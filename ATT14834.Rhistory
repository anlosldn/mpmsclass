library(tm)
api_key <- "mDTvXFnSYgvalSxdjPpSexOzk"
api_secret <- "YVquCgczuH1j1By6lvTehL68vySSmNAy692ahfQM60IDAvQ4iT"
access_token <- "995992638422691841-UZeuk7QC1itKghpP0K5sm7DrCaNcldV"
access_token_secret <- "jZh7VyCZIrKi0ESsp60jz9pzmvhvANcLw6YC5BaRPqxjh"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
install.packages(c("devtools","rjson","bit64","httr"))
install.packages("ROAuth")
install.packages("twitteR")
install.packages("RCurl")
install.packages("tm")
install.packages("wordcloud")
install.packages("quanteda")
library(devtools)
library(twitteR)
library(ROAuth)
library(RCurl)
library(twitteR)
library(httr)
library(tm)
install.packages("tm")
api_key <- "mDTvXFnSYgvalSxdjPpSexOzk"
api_secret <- "YVquCgczuH1j1By6lvTehL68vySSmNAy692ahfQM60IDAvQ4iT"
access_token <- "995992638422691841-UZeuk7QC1itKghpP0K5sm7DrCaNcldV"
access_token_secret <- "jZh7VyCZIrKi0ESsp60jz9pzmvhvANcLw6YC5BaRPqxjh"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
BTC_tweets = searchTwitter("bitcoin", n=1000)
# we now create BTC function
BTClist <- sapply(BTC_tweets, function(x) x$getText())
# use the corpus function to create a text body of all the text in the BTC object
BTCcorpus <- Corpus(VectorSource(BTClist))
# convrt all text to lower case
BTCcorpus <- tm_map(BTCcorpus, tolower)
#remove punctuation
BTCcorpus <- tm_map(BTCcorpus, removePunctuation)
# remove stopwords, unneccessary words
BTCcorpus <- tm_map(BTCcorpus,
function(x)removeWords(x,stopwords()))
library(wordcloud)
wordcloud(BTCcorpus)
wordcloud(BTCcorpus, min.freq = 4, scale = c(5,2),
random.color = T, max.words = 30, random.order = F)
wordcloud(BTCcorpus, min.freq = 4, scale = c(5,2),
random.color = T, max.words = 50, random.order = F)
BTCtdm <- TermDocumentMatrix(BTCcorpus)
BTCtdm
findFreqTerms(BTCtdm, lowfreq = 30)
findFreqTerms(BTCtdm, lowfreq = 10)
findFreqTerms(BTCtdm, lowfreq = 30)
# Find Associations
findAssocs(BTCtdm, 'design', 0.6)
BTCtdm
# Find frequent terms
findFreqTerms(BTCtdm, lowfreq = 40)
# Find Associations
findAssocs(BTCtdm, 'blocks', 0.6)
# Find Associations
findAssocs(BTCtdm, 'blocks', 0.6)
# Find Associations
findAssocs(BTCtdm, 'halving', 0.6)
# Find Associations
findAssocs(BTCtdm, 'news', 0.6)
# Find Associations
findAssocs(BTCtdm, 'covid', 0.6)
# Dendrogram - which words are related?
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.9)
head(BTCtdm_2)
summary(BTCtdm_2)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# Dendrogram - which words are related?
# remove infrequent words
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.7)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# Dendrogram - which words are related?
# remove infrequent words
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.8)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# Dendrogram - which words are related?
# remove infrequent words
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.9)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# Dendrogram - which words are related?
# remove infrequent words
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.85)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# Dendrogram - which words are related?
# remove infrequent words
BTCtdm_2 <- removeSparseTerms(BTCtdm, sparse = 0.95)
BTCtdm_2
# Scale the data
BTCtdm_2scale <- scale(BTCtdm_2)
BTC_dist <- dist(BTCtdm_2scale, method = 'euclidean')
# hierarchical clustering
BTC_fit <- hclust(BTC_dist)
plot(BTC_fit)
# how to cluster to a certain number of groups
cutree(BTC_fit, k=5)
rect.hclust(BTC_fit, k=5, border = "red")
# how to cluster to a certain number of groups
cutree(BTC_fit, k=6)
rect.hclust(BTC_fit, k=6, border = "red")
install.packages("sentimentr")
library(sentimentr)
install.packages("ndjson")
library(ndjson)
df = stream_in("AmazonBeauty.json")
head(df)
install.packages("fpp2")
rm(list=ls())  # clear environment
install.packages('fpp2')
library(fpp2)
plot(goog200)
plot(diff(goog200))
Box.test(goog200, lag=10, type = "Ljung-Box")
library(tseries)
adf.test(goog200)
Box.test(diff(goog200), lag=10, type = "Ljung-Box")
adf.test(diff(goog200))
## Example of secondary differencing - transforming to log scale, then seasonal differencing
cbind("Sales ($mil)" = a10,
"Monthly log sales" = log(a10),
"Annual change in log sales" = diff(log(a10),12)) %>%
autoplot(facets=TRUE)+
xlab("Year")+ylab(" ")+
ggtitle("Antidiabetic drug sales")
## Second Example
cbind("Billion kWh" = usmelec,
"Logs" = log(usmelec),
"Seasonally\n differenced logs" = diff(log(usmelec),12),
"Doubly\n differenced logs" = diff(diff(log(usmelec),12),1)) %>%
autoplot(facets=TRUE) +
xlab("Year")+ylab(" ")+
ggtitle("Monthly US Elec Generation")
## Unit Root Test ##
library(urca)
goog %>% ur.kpss() %>% summary()
adf.test(goog)
## now run the test on the differenced data
goog %>% diff() %>% ur.kpss() %>% summary()
adf.test(diff(goog))
## How many differences do we need to do?
plot(diff(goog))
ndiffs(goog)
##Seasonal differencing required?
usmelec %>% log() %>% nsdiffs()
autoplot(uschange[,"Consumption"]) +
xlab("Year") + ylab("Quarterly % change")
ndiffs(uschange)
fit <- auto.arima(uschange[,"Consumption"], seasonal=F, stepwise = F,
approximation = F, trace = T)
fit
# c = 0.745 x (1 - 0.589) = 0.307
# e = white noise with std dev = sqrt(0.3499)
accuracy(fit)
# generate random observations from the two populations
x <- rnorm(10, mean=100, sd=5) # normal distribution centered at 100
y <- rnorm(20, mean=105, sd=5) # normal distribution centered at 105
# Student's t-test
t.test(x, y, var.equal=TRUE) # run the Student's t-test
install.packages("hexbin")
install.packages(c("quantmod","rugarch","rmgarch",
"tidyverse","zoo","forecast","Quandl"))
library(quantmod) # this allows us to download stock data directly into R
library(rugarch)
library(rmgarch)
library(tidyverse)
library(zoo)
library(forecast)
library(ggplot2)
library(Quandl)
#Import Corn Prices
df <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
# Convert df into a time series object
df1 <- xts(df$Last, as.Date(df$Date, format = "%d/%m/%Y"))
df1 = na.approx(df1) # fills in any missing data points
class(df1)
plot(df1, type = "l", main = "Continuous Near Dated Corn Futures")
df5 <- df1["2020-01-01/2020-05-18"]
plot(df5, main="Corn Price per Bushel in 2020")
# So clearly we need to transform our data (first differencing)
rCorn <- dailyReturn(df1["2017-05-18/2020-05-18"])
plot(rCorn)
# we typicaly build several GARCH models and then cross validate them
# we use the ugarchspec()
ug_spec = ugarchspec()
ug_spec # gives us the model defaults
# Base Model
GARCHModel_1 = ugarchfit(spec = ug_spec, data = rCorn) # this is a default sGARCH s=standard
# Model 2
ug_spec2 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(0,0)),distribution.model = "std")
GARCHModel_2 = ugarchfit(spec = ug_spec2, data = rCorn)
# Model 3
ug_spec3 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(2,2)),distribution.model = "std")
GARCHModel_3 = ugarchfit(spec = ug_spec3, data = rCorn)
GARCHModel_3 # AIC = -5.9878
## We settle on GARCHModel_2 as our best model
CornFuturesPredict <- ugarchboot(GARCHModel_2, n.ahead = 10,
method = c("Partial","Full")[1]) # 10 days ahead
plot(CornFuturesPredict) # 2 = prices, 3 = volatility
ts_nn <- df1["2016-05-18/2020-05-18"]
plot(ts_nn)
set.seed(1) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(ts_nn, p=2)
nnetforecast <- forecast(fit, h=270, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
plot(CornFuturesPredict, type=2)# 2 = prices, 3 = volatility
# this plots the series - the forecast price
plot(Corn_Predict, which=2)
install.packages("Quandl")
library(Quandl)
data <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
data %>% head()
data <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
data %>% head()
ts <- xts(data$Last, as.Date(data$Date, format = "%d/%m/%Y"))
plot(ts)
myts <- ts["2016-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
library(ggplot2)
autoplot(myts, main="Near Dated Corn Futures Prices")
myts = na.approx(myts)
myts %>% tail()
tail(myts)
set.seed(10) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
library(forecast)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
rm(list=ls())
install.packages(c("quantmod","rugarch","rmgarch",
"tidyverse","zoo","forecast","Quandl"))
install.packages(c("quantmod", "rugarch", "rmgarch", "tidyverse", "zoo", "forecast", "Quandl"))
library(quantmod) # this allows us to download stock data directly into R
library(rugarch)
library(rmgarch)
library(tidyverse)
library(zoo)
library(forecast)
library(ggplot2)
library(Quandl)
#Import Corn Prices
df <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
# Convert df into a time series object
df1 <- xts(df$Last, as.Date(df$Date, format = "%d/%m/%Y"))
df1 = na.approx(df1) # fills in any missing data points
class(df1)
plot(df1, type = "l", main = "Continuous Near Dated Corn Futures")
df5 <- df1["2020-01-01/2020-05-18"]
plot(df5, main="Corn Price per Bushel in 2020")
# So clearly we need to transform our data (first differencing)
rCorn <- dailyReturn(df1["2017-05-18/2020-05-18"])
plot(rCorn)
plot(rCorn, main="Corn Futures Daily Price Moves since May 2017")
# So clearly we need to transform our data (first differencing)
rCorn <- dailyReturn(df1["2017-05-18/2020-05-18"])
plot(rCorn, main="Corn Futures Daily Price Moves since May 2017")
# we typicaly build several GARCH models and then cross validate them
# we use the ugarchspec()
ug_spec = ugarchspec()
ug_spec # gives us the model defaults
# Base Model
GARCHModel_1 = ugarchfit(spec = ug_spec, data = rCorn) # this is a default sGARCH s=standard
# Model 2
ug_spec2 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(0,0)),distribution.model = "std")
GARCHModel_2 = ugarchfit(spec = ug_spec2, data = rCorn)
# Model 3
ug_spec3 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(2,2)),distribution.model = "std")
GARCHModel_3 = ugarchfit(spec = ug_spec3, data = rCorn)
GARCHModel_3 # AIC = -5.9878
## We settle on GARCHModel_2 as our best model
CornFuturesPredict <- ugarchboot(GARCHModel_2, n.ahead = 10,
method = c("Partial","Full")[1]) # 10 days ahead
plot(CornFuturesPredict, which = 2)
plot(CornFuturesPredict, which = 3) # forecast volatility
myts <- df1["2016-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
myts <- df1["2015-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
autoplot(myts, main="Near Dated Corn Futures Prices - Last 5 Years")
#myts = na.approx(myts)
tail(myts)
set.seed(250) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=10)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=100)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=50)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=20)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
nnetforecast <- forecast(fit, h=90, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
fit1 <- nnetar(myts, p=3)
nnetforecast1 <- forecast(fit1, h=90, PI=F)
autoplot(nnetforecast1)
nnetforecast2 <- forecast(fit2, h-90, PI=F)
# for comparison
fit2 <- nnetar(myts)
nnetforecast2 <- forecast(fit2, h-90, PI=F)
nnetforecast2 <- forecast(fit2, h=90, PI=F)
autoplot(nnetforecast2)
getwd()
install.packages(c("quantmod","rugarch","rmgarch",
"tidyverse","zoo","forecast","Quandl"))
install.packages(c("quantmod", "rugarch", "rmgarch", "tidyverse", "zoo", "forecast", "Quandl"))
Quandl("CHRIS/CME_CSC10", api_key="nQW1ynmsFtb42YM2sf4Z")
install.packages("Quandl")
library(Quandl)
datacheese = Quandl("CHRIS/CME_CSC10", api_key="nQW1ynmsFtb42YM2sf4Z")
datacheese
datacheese %>% head()
head(datacheese)
class(datacheese)
df7 <- xts(datacheese$Last, as.Date(datacheese$Date, format = "%d/%m/%Y"))
plot(df7)
plot(df7)
df7 %>% tail()
tail(df7)
df8 <- xts["01/11/2018"/"22/05/2020"]
df8 <- xts["01/11/2018-22/05/2020"]
df8 <- xts["01-11-2018/22-05-2020"]
df8 <- df7["01-11-2018/22-05-2020"]
install.packages("Quandl")
install.packages("Quandl")
library(Quandl)
library(ggplot2)
library(forecast)
data <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
data %>% head()
class(data)
ts <- xts(data$Last, as.Date(data$Date, format = "%d/%m/%Y"))
plot(ts)
myts <- ts["2016-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
tail(myts)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
myts = na.approx(myts)
tail(myts)
set.seed(10) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
myts <- ts["2019-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
tail(myts)
myts = na.approx(myts)
tail(myts)
set.seed(10) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=3)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts)
nnetforecast <- forecast(fit, h=180, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
nnetforecast <- forecast(fit, h=10, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
myts <- ts["2018-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices")
tail(myts)
myts = na.approx(myts)
tail(myts)
set.seed(10) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts)
nnetforecast <- forecast(fit, h=10, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=5)
nnetforecast <- forecast(fit, h=10, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
rm(list=ls())
setwd("C:\Users\gary_\Google Drive (gary.watson.2801@gmail.com)\BPP\M&P Market Systems\R\Shared")
# select directory in windows, copy PATH, then run before pasting...(takes care of Windows backslash)
x <- gsub("\\\\", "/",readClipboard())
setwd(x) #set working directory
getwd()
install.packages(c("quantmod","rugarch","rmgarch",
"tidyverse","zoo","forecast","Quandl"))
install.packages(c("quantmod", "rugarch", "rmgarch", "tidyverse", "zoo", "forecast", "Quandl"))
library(quantmod) # this allows us to download stock data directly into R
library(rugarch)
library(rmgarch)
library(tidyverse)
library(zoo)
library(forecast)
library(ggplot2)
library(Quandl)
#Import Corn Prices
df <- Quandl("CHRIS/CME_C1", api_key="nQW1ynmsFtb42YM2sf4Z")
# Convert df into a time series object
df1 <- xts(df$Last, as.Date(df$Date, format = "%d/%m/%Y"))
df1 = na.approx(df1) # fills in any missing data points
class(df1)
plot(df1, type = "l", main = "Continuous Near Dated Corn Futures")
df5 <- df1["2020-01-01/2020-05-18"]
plot(df5, main="Corn Price per Bushel in 2020")
ndiffs(df5)
# So clearly we need to transform our data (first differencing)
rCorn <- dailyReturn(df1["2017-05-18/2020-05-18"])
plot(rCorn, main="Corn Futures Daily Price Moves since May 2017")
# we typicaly build several GARCH models and then cross validate them
# we use the ugarchspec()
ug_spec = ugarchspec()
ug_spec # gives us the model defaults
# Base Model
GARCHModel_1 = ugarchfit(spec = ug_spec, data = rCorn) # this is a default sGARCH s=standard
# Model 2
ug_spec2 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(0,0)),distribution.model = "std")
GARCHModel_2 = ugarchfit(spec = ug_spec2, data = rCorn)
# Model 3
ug_spec3 = ugarchspec(variance.model = list(model="sGARCH", garchOrder=c(1,1)),mean.model = list(armaOrder=c(2,2)),distribution.model = "std")
GARCHModel_3 = ugarchfit(spec = ug_spec3, data = rCorn)
GARCHModel_3 # AIC = -5.9878
## We settle on GARCHModel_2 as our best model
CornFuturesPredict <- ugarchboot(GARCHModel_2, n.ahead = 10,
method = c("Partial","Full")[1]) # 10 days ahead
plot(CornFuturesPredict, which = 2) # forecats prices
plot(CornFuturesPredict, which = 3) # forecast volatility
## We settle on GARCHModel_2 as our best model
CornFuturesPredict <- ugarchboot(GARCHModel_2, n.ahead = 10,
method = c("Partial","Full")[1]) # 10 days ahead
CornFuturesPredict
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=20)
nnetforecast <- forecast(fit, h=90, PI=F) # takes too long to run the prediction intervals
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=20)
myts <- df1["2015-05-18/2020-05-18"]
autoplot(myts, main="Near Dated Corn Futures Prices - Last 5 Years")
set.seed(250) # there is a random element in NNAR model, set seed to get same result
# Creates a neural net model - automatically select parameters
fit <- nnetar(myts, p=20)
nnetforecast <- forecast(fit, h=90, PI=F) # takes too long to run the prediction intervals
library(ggplot2)
autoplot(nnetforecast)
# for comparison
fit1 <- nnetar(myts, p=3)
nnetforecast1 <- forecast(fit1, h=90, PI=F)
autoplot(nnetforecast1)
nnetforecast1 <- forecast(fit1, h=90, PI=F)
autoplot(nnetforecast1)
# for comparison
fit2 <- nnetar(myts)
nnetforecast2 <- forecast(fit2, h=90, PI=F)
autoplot(nnetforecast2)
